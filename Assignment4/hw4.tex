\documentclass[12pt]{article}%
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.2cm, right=2.2cm]%
{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage{stfloats}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{indentfirst}
\setlength{\parindent}{2em}
\setcounter{MaxMatrixCols}{30}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\usepackage{mathtools}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

\begin{document}

\title{MATH2040C Homework 4}
\author{ZHENG Weijia (William, 1155124322)}
\date{\today}
\maketitle



\section{Section 5.1, Q2(e)}

Given that $\beta = \{1-x+x^3,1+x^2,1,x+x^2\}.$ 

And note that $T(1-x+x^3)=-1+x-x^3.$ $T(1+x^2)=-x-x^2+x^3.$ $T(1)=x^2.$ $T(x+x^2)=-x-x^2.$

Hence $T(\beta)=\{-1+x-x^3,-x-x^2+x^3,x^2, -x-x^2\}.$

$[T]_\beta =\begin{pmatrix}-1&1&0&0\\0&-1&1&0\\0&0&-1&0\\0&0&0&-1\end{pmatrix}.$

Suppose $\beta$ is containing $T$'s eigenvectors, then $\exists \lambda \in F$ such that $$T(1+x^2)=\lambda(1+x^2).$$

Then $\lambda + \lambda x^2 = -x-x^2+x^3.$ Note that the degree of them do not equal in any sense. 

Hence $\beta$ is not a basis consisting of eigenvectors of $T.$

~\ 

\section{Section 5.1, Q2(f)}
Given that $\beta = \{\begin{pmatrix}1&0\\1&0\end{pmatrix},\begin{pmatrix}-1&2\\0&0\end{pmatrix},\begin{pmatrix}1&0\\2&0\end{pmatrix},\begin{pmatrix}-1&0\\0&2\end{pmatrix}\}.$

Note that $T\begin{pmatrix}1&0\\1&0\end{pmatrix}=\begin{pmatrix}-3&0\\-3&0\end{pmatrix}=-3\begin{pmatrix}1&0\\1&0\end{pmatrix},$

$T\begin{pmatrix}-1&2\\0&0\end{pmatrix}=\begin{pmatrix}-1&2\\0&0\end{pmatrix}=1\cdot \begin{pmatrix}-1&2\\0&0\end{pmatrix},$ 

$T\begin{pmatrix}1&0\\2&0\end{pmatrix}=\begin{pmatrix}1&0\\2&0\end{pmatrix}=1\cdot \begin{pmatrix}1&0\\2&0\end{pmatrix},$ and 

$T\begin{pmatrix}-1&0\\0&2\end{pmatrix}=\begin{pmatrix}-1&0\\0&2\end{pmatrix}=1\cdot \begin{pmatrix}-1&0\\0&2\end{pmatrix}.$

Hence we deduce that $\beta$ is a basis consisting of eigenvectors of $T$.

~\ 

\section{Section 5.1, Q3(d)}
\subsection{(i)}
Given that $A = \begin{pmatrix}2&0&-1\\4&1&-4\\2&0&-1\end{pmatrix}$, then its characteristic polynomial is $$f_{A}(t)=\det{\begin{pmatrix}2-t&0&-1\\4&1-t&-4\\2&0&-1-t\end{pmatrix}}=-t(t-1)^2.$$

Observe the $f_{A}(t)$'s zeros, we have A should have 2 eigenvalues: 1 and 0.

\subsection{(ii)}
For eigenvalue 1, the corresponding eigenvectors should be in the span of set $$\{\begin{pmatrix}1\\0\\1\end{pmatrix},\begin{pmatrix}0\\1\\0\end{pmatrix}\}.$$

For eigenvalue 0, the corresponding eigenvectors should be in the span of set $$\{\begin{pmatrix}1\\4\\2\end{pmatrix}\}.$$

\subsection{(iii)}
In this case, the $n=3, F = \mathbb{R}.$ So $F^3 = \mathbb{R}^3.$

Note that $\{ \begin{pmatrix}1\\0\\1\end{pmatrix}, \begin{pmatrix}0\\1\\0\end{pmatrix}, \begin{pmatrix}1\\4\\2\end{pmatrix}     \}$ is a 3-linear-indenpendent set. Hence it is a basis of $\mathbb{R}^3.$ And by our conclusion above, these 3 vectors are eigenvectors of $A$.

\subsection{(iv)}
Let $Q=\begin{pmatrix}1&0&1\\0&1&4\\1&0&2\end{pmatrix}$. Note that $Q$ is invertible and $Q^{-1}=\begin{pmatrix}2&0&-1\\4&1&-4\\-1&0&1\end{pmatrix}.$ 

Note that $$Q^{-1}AQ =\begin{pmatrix}1&0&0\\0&1&0\\0&0&0\end{pmatrix}.$$ 

~\ 

\section{Section 5.1, Q4(h)}
Let $\beta$ be the standard basis.
Note that $[T]_{\beta}= \begin{pmatrix}0&0&0&1\\0&1&0&0\\0&0&1&0\\1&0&0&0\end{pmatrix}.$ By extracting its characteristic polynomial, it is $$f_{T}(t)=(t-1)^3(t+1)=0.$$

And note that their corresponding eigenvectors to be $\{\begin{pmatrix}0\\1\\0\\0\end{pmatrix},\begin{pmatrix}0\\0\\1\\0\end{pmatrix},\begin{pmatrix}1\\0\\0\\1\end{pmatrix},\begin{pmatrix}1\\0\\0\\-1\end{pmatrix}\}.$

Note that by the diagnoalizability of $[T]_\beta$, (for its every eigenvalue: 1 and -1: algebraic multiplicity equals geometric multiplicity) we have $$\begin{pmatrix}1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&-1\end{pmatrix}=Q^{-1}[T]_\beta Q.$$

Where $Q=\begin{pmatrix}0&0&-1&1\\-1&0&0&0\\0&1&0&0\\0&0&-1&-1\end{pmatrix}.$

Regard Q as a change of basis matrix from another basis $\gamma$ to our known standard basis $\beta.$ Therefore, $Q=[I]_{\gamma}^{\beta}.$

Let $\gamma=\{y_1,y_2,y_3,y_4\}.$ Therefore $$[y_1,y_2,y_3,y_4]_\beta =\begin{pmatrix}0&0&-1&1\\-1&0&0&0\\0&1&0&0\\0&0&-1&-1\end{pmatrix}. $$

Hence, $y_1 = \begin{pmatrix} 0&-1\\0&0\end{pmatrix}$, $y_2 = \begin{pmatrix}0&0\\1&0\end{pmatrix}$, $y_3 = \begin{pmatrix} -1&0\\0&-1\end{pmatrix}$, $y_4 = \begin{pmatrix} 1&0\\0&-1\end{pmatrix}.$

Note that $[T]_\gamma=\begin{pmatrix}0&0&-1&1\\-1&0&0&0\\0&1&0&0\\0&0&-1&-1\end{pmatrix}.$ So $\gamma$ is the ordered basis we need to find.

~\ 

\section{Section 5.1, Q4(e)}

Let $\beta = \{1+x, -3-13x+4x^2, -3+x\}$ be a ordered basis. Then $$[T]_\beta=[4x+4,8x^2-26x-6,0]_\beta=\begin{pmatrix}4&0&0\\0&2&0\\0&0&0\end{pmatrix}.$$

Which is a diagnoal matrix. Hence the $\beta$ is what we want to find. And the eigenvalues of $T$ are 4,2 and 0, with corresponding eigenvectors elements of the ordered basis $\beta.$

~\ 

\section{Section 5.1, Q17}
\subsection{(a)}
Note that for the identity matrix I, $T(I)=I=1\cdot I$, hence 1 is a eigenvalue. 
Also note that for a matrix X with only 2 entries on the right top and left buttom being 1 and -1, then $$T(X)=-X.$$ Hence -1 is a eigenvalue.

Suppose there exists some eigenvalue $|\lambda| \neq 1$ such that $$A^{T}=\lambda A.$$ Then we can deduce $\lambda A^{T}=A=\lambda^{2}A.$ Which implies $(1-\lambda^2)A=0.$ 

Because A is regarded as an eigenvector, hence it is not zero, so $1-\lambda^2$ must be 0. But other than 1 and -1, it cannot be 0.

Hence 1, -1 are the only eigenvalues of A.

\subsection{(b)}

For eigenvalue 1, the corresponding eigenvectors are all symmetric matrices. 

For eigenvalue -1, the corresponding eigenvectors are all skew symmetric matrices.

\subsection{(c)}
Consider $\gamma=\{\begin{pmatrix} 1&0\\0&0\end{pmatrix}, \begin{pmatrix} 0&1\\0&0\end{pmatrix}, \begin{pmatrix} 0&0\\1&0\end{pmatrix}, \begin{pmatrix} 0&0\\0&1\end{pmatrix}\}.$

Then $[T]_\gamma = \begin{pmatrix} 1&0&0&0\\0&0&1&0\\0&1&0&0\\0&0&0&1\end{pmatrix}.$

Note that $$\begin{pmatrix} 1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&-1\end{pmatrix}=Q^{-1}[T]_\gamma Q,$$ where $Q=\begin{pmatrix} 0&0&1&0\\0&1&0&1\\0&1&0&-1\\1&0&0&0\end{pmatrix}.$

Where Q can be regarded as $[I]_{\beta}^{\gamma}.$ Let $\beta=\{v_1,v_2,v_3,v_4\}.$ $$[I]_{\beta}^{\gamma}=[v_1,v_2,v_3,v_4]_\gamma=[\begin{pmatrix} 0&0\\0&1\end{pmatrix},\begin{pmatrix} 0&1\\1&0\end{pmatrix},\begin{pmatrix} 1&0\\0&0\end{pmatrix},\begin{pmatrix} 0&1\\-1&0\end{pmatrix}].$$

Hence the $\gamma$ is the basis we want.

\subsection{(d)}
Note that for 1 as an eigenvalue, its corresponding eigenvectors are $$E_1 = \{e_{1ij},i\leq j\}.$$

Where the $e_{1ij}$ are defined as a matrix with i-j's slot to be 1 and j-i's slot also being 1, while others remains 0.

And for -1 as an eigenvalue, its corresponding eigenvectors are $$E_{-1} = \{e_{2ij},i<j\}.$$ Where $e_{2ij}$ is defined as a matrix with i-j's slot being 1 and j-i slot being -1.

Then, take $\gamma=\{e_{ij},1\leq i,j \leq n\}.$ Take $[E_1, E_{-1}]_\gamma$ as a change of order matrix.

If we regard the $E_1, E_{-1}$ as a basis of $M_{n \times n}(\mathbb{R})$, then we can regard $$[E_1, E_{-1}]_\gamma=[I(v_1),I(v_2),...,I(v_{n^2})]_\gamma.$$

Note that the eigenvectors inside $E_1,E_{-1}$ are all linear indenpendent, and there are $\frac{n(n-1)}{2}+n + \frac{n(n-1)}{2}=n^2=\dim{M_{n \times n}(\mathbb{R})}.$ Hence $[E_1,E_{-1}]$ is a basis.

Than note that $$\begin{pmatrix} \lambda_1&...&...&0\\0&\lambda_2&...&0\\...&...&...&...\\0&...&...&\lambda_n\end{pmatrix} = ([E_1,E_{-1}]_{\beta}^{\gamma})^{-1} [T]_{\gamma}[E_1,E_{-1}]_{\beta}^{\gamma}$$ where $|\lambda_i|=1, \forall i.$

Deifine the $\beta = \{v_1,v_2,...,v_{n^2}\}=$ \{ every column of $ E_1, E_{-1}\}.$

Hence the $\beta$ is the basis we want to find.



~\ 

\section{Section 5.1, Q18}
\subsection{(a)}
If $A$ is not invertible, then let $c=0.$ We have $$\det{(A+cB)}=\det{A}=0.$$ Since $A$ is singular as we supposed.

If $A$ is invertible, then note that $A = AB^{-1}B$, then $$\det{(A+cB)}=\det{AB^{-1}B+cB}=\det{(AB^{-1}+cI)}\det{(B)}.$$

Note that $\det{(B)}\neq 0$ and $\det{(AB^{-1}+cI)}=0$ if $-c$ is the eigenvalue of $AB^{-1}$. And by the fundemantal theorem of algebra, there must exist such c.

Done.

\subsection{(b)}
Let $A=\begin{pmatrix}2&1\\0&1\end{pmatrix}$ and $B=\begin{pmatrix}0&1\\0&0\end{pmatrix}.$

Note that $\det{(A)}=2\neq 0.$ And $\forall c\in \mathbb{C},$ $$\det{(A+cB)}=\det{(\begin{pmatrix}2&1+c\\0&1\end{pmatrix})}=2\neq 0.$$

Therefore, A and A+cB are both invertible. 
~\ 





\section{Section 5.2, Q3(c)}

Note that $V=\mathbb{R}^3.$

Define $\gamma=\{ \begin{pmatrix}1\\0\\0\end{pmatrix} , \begin{pmatrix}0\\1\\0\end{pmatrix}, \begin{pmatrix}0\\0\\1\end{pmatrix} \}$ and hence $T(\gamma)=\{ \begin{pmatrix}0\\-1\\0\end{pmatrix}, \begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}0\\0\\2\end{pmatrix} \}$
And we can see that the characteristic polynomial of $[T]_\gamma=\begin{pmatrix}0&1&0\\-1&0&0\\0&0&2\end{pmatrix}$ is $$f_{T}(t)=(2-t)(t^2+1).$$

Which only have one eigenvalue in $\mathbb{R}.$ Hence it only have not enough eigenvalues, hence not diagnoalizabile.




~\ 


\section{Section 5.2, Q8}

$1\leq \gamma_{T}(\lambda_1) = \dim{(E_{\lambda_1})}=n-1.$ 

$1\leq \gamma_{T}(\lambda_2) = \dim{(E_{\lambda_2})}.$

Take $n-1$ orthogonal and eigenvectors from $E_{\lambda_1}$ denote them as $v_1,v_2,...,v_{n-1}.$

And then take 1 eigenvector from $E_{\lambda_2}$, make it orthogonal to $v_1,v_2,...,v_{n-1}$ and denote it as $w.$

Consider $Q= \begin{pmatrix}v_1 & v_2 & ... & v_n & w\end{pmatrix}.$ Note that $Q^{-1}=Q^{T}.$ 

Because $Q^{T}Q=\begin{pmatrix}v_1 \\ v_2 \\ ... \\ v_n \\ w\end{pmatrix} \begin{pmatrix}v_1 & v_2 & ... & v_n & w\end{pmatrix}= I.$ Because every $v_i,w$ are orthogonal.

Then, note that $$Q^{-1}AQ=Q^{T}[\lambda_1 v_1,\lambda_1 v_2,...,\lambda_1 v_{n-1}, \lambda_2 w ]=\begin{pmatrix}v_1 \\ v_2 \\ ... \\ v_n \\ w\end{pmatrix} [\lambda_1 v_1,\lambda_1 v_2,...,\lambda_1 v_{n-1}, \lambda_2 w ]$$.

Which is that $Q^{-1}AQ=\begin{pmatrix} \lambda_1&0&...&0\\0&\lambda_1&...&...\\...&...&...&...\\0&...&...&\lambda_2 \end{pmatrix}.$

Therefore, A is diagnoalizabile.


~\ 


\section{Section 5.2, Q13}
\subsection{(a)}
Consider matrix $A =\begin{pmatrix}1&2\\1&0\end{pmatrix}.$ We know that A has 2 eigenvalues 2 and -1, with corresponding eigenvectors $\begin{pmatrix} 2\\ 1\end{pmatrix}$ and $\begin{pmatrix} -1\\1\end{pmatrix}.$

And $A^{T}=\begin{pmatrix}1&1\\2&0\end{pmatrix},$ while having a eigenvalue 2, its corresponding eigenvector is $\begin{pmatrix} 1\\ 1\end{pmatrix}.$ So the eigenbasis $E_2$ of $A$ and $A^{T}$ are not the same for sure.


\subsection{(b)}




~\ 

\end{document}
